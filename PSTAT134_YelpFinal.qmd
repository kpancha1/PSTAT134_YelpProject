---
title: "PSTAT134_Yelp_Final"
format: pdf
editor: visual
---

# Final Project: NLP on Yelp Reviews

## PSTAT 134 - Group 19

## Introduction

This project utilizes a precompiled dataset from Yelp known as the Yelp Open Dataset. This dataset is made for academic use and research. The data is in JSON format and includes 6,990,280 reviews from 150,346 businesses over 11 metropolitan areas. Additionally, there are about 900,000 tips from over a million users which include over 1.2 million business attributes like hours, parking, availability, and ambience. The goal of this data science project is to utilize natural language processing and perform exploratory data analysis to gain insights that can help reveal patterns and trends between reviewers/reviews and business star ratings, votes, business categories, and more. We are interested in finding what insights from reviews are impactful in determining review star ratings and thereby business star ratings.

## Methods

Due to the dataset being in JSON format, initial data cleaning was required. The file, \`get_data.R\`, retrieves the JSON file and converts the data into a more readable format as multiple CSV files. The script processes the JSON file in chunks of 1,000 lines and saves processed chunks of data into .rds files every 1,000,000 lines to prevent memory overload. 

The file, \`dataset.R\`, is the script in which the CSV files are unified and unnecessary variables are dropped, such as the \`friends\` column.  This file results in a final and cleaned \`dataset.csv\` file with over one million observations (reviews). The variables in this file are user_id, business_id, review_stars, vote_useful, vote_funny , vote_cool, text, city, business_stars, business_review_count, categories, user_review_count, yelping_since, user_useful, user_funny, user_cool, elite, fans, average_stars, compliment_hot, compliment_more, compliment_profile, compliment_cute, compliment_list compliment_note, compliment_plain, compliment_cool, compliment_funny, compliment_writer compliment_photos, tip_compliment_count, total_vote, and total_compliments.

### Setup

#### Installing Packages

```{r, results='hide'}
rm(list=ls())
gc() 
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(stringr)
library(igraph)
library(ggraph)
library(tidytext)
library(wordcloud)
library(stringr)
library(textTinyR)
library(tm)
library(e1071)
library(tidyr)
library(caret)
library(naniar)
library(data.table)
library(shiny)
library(htmltools)
library(kableExtra)
library(tidyr)


```

#### Load in Dataset

```{r}
yelp <- fread("cleaned_dataset.csv")
```

### Checking NA values

```{r}
vis_miss(yelp, warn_large_data = FALSE) 
```

/////commetns

### Distribution of Reviewers' Star Ratings

Let's take a look at the entire dataset's distribution of `review_stars`, the number of stars the reviewer gave the business.

```{r}
yelp %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +  
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.3) +
  theme_minimal()
```

This dataset has almost seven million observations. To work with the data, we will take a random sample of $10,000$ reviews, stratified by `review_stars`.

We stratify on `review_stars` to ensure that we are able to capture a representative distribution of reviews across all star ratings. Since `review_stars` reflects the sentiment or quality of the review (ranging from 1 star for very negative to 5 stars for very positive), stratifying by this variable guarantees that every rating level is proportionally included in the sample. This avoids over- or under-representing any particular rating category in the sample.

```{r}
yelp_sample <- yelp %>% 
  group_by(review_stars) %>%
  sample_frac(1000000 / nrow(yelp)) %>%
  ungroup()
```

We see that the proportion below remains the same. We can now begin with data cleaning of the working dataset, `yelp_sample`.

```{r}
yelp_sample %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.3) +
  theme_minimal()

```


### Data Cleaning

Observing the csv file, we see that the `yelping_since` column give the specific date and time at which the user created their Yelp profile. We do not require such detailed information for our analysis so we will keep only the year the user joined Yelp.

```{r}
yelp_sample <- yelp_sample %>%
  mutate(yelping_since = substr(yelping_since, 1, 4)) # get only the year
```

We also want to clean the actual reviews so that we are able to properly perform NLP analysis on the text. That is done below by removing punctuation, symbols, extra white space, adding a space before capital letters, and replacing all uppercase with lowercase letters.

```{r}
# cleaning the reviews (text column)
remove <- c('[[:punct:]]', 
            '[[:digit:]]', 
            '[[:symbol:]]',
            'im', 'ive') %>%
  paste(collapse = '|')

yelp_sample$text <- yelp_sample$text %>% 
  str_remove_all('\'') %>%
  str_replace_all(remove, ' ') %>%
  str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
  tolower() %>%
  str_replace_all("\\s+", " ")
```

#### Removing Stop Words

```{r}
# using tidytext stop words lexicon
data("stop_words")

yelp_sample %>% 
  unnest_tokens(word, text) %>% # tokenizing reviews into words
  anti_join(stop_words) %>% # removing stop words
  count(word, sort = TRUE) %>% 
  head(n = 30) %>% 
  kbl() %>% 
  scroll_box(width = "400px", height = "500px")
```

#### Most Common Words

#coment on common words

```{r}
yelp_sample_tokenized <- yelp_sample %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

word_counts <- yelp_sample_tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 10000) %>%
  mutate(word = reorder(word, n))

ggplot(word_counts[1:20,], aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

```

```{r}
word_counts_by <- yelp_sample_tokenized %>%
  count(review_stars, word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder_within(word, n, review_stars))
  
word_counts_1to3 <- word_counts_by %>%
  filter(review_stars %in% 1:3)

word_counts_4to5 <- word_counts_by %>%
  filter(review_stars %in% 4:5)

plot_1to3 <- ggplot(word_counts_1to3[1:40, ], aes(x = n, y = word, fill = review_stars)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review_stars, scales = "free_y", 
             labeller = labeller(review_stars = function(x) paste(x, "Stars"))) +
  scale_y_reordered() +
  labs(title = "Most Common Words for Ratings 1 to 3",
       x = "Frequency",
       y = NULL) +
  theme_minimal()

plot_4to5 <- ggplot(word_counts_4to5[1:40, ], aes(x = n, y = word, fill = review_stars)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review_stars, scales = "free_y", 
             labeller = labeller(review_stars = function(x) paste(x, "Stars"))) +
  scale_y_reordered() +
  labs(title = "Most Common Words for Ratings 4 to 5",
       x = "Frequency",
       y = NULL) +
  theme_minimal()

print(plot_1to3)
print(plot_4to5)

```

#### Word Cloud

#comment on the positive and negative meaning

```{r}
sentiment_counts <- yelp_sample_tokenized %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

comparison.cloud(sentiment_counts,
                 colors = c("red", "blue"),
                 max.words = 50, 
                 scale = c(2.5, 0.5),
                 random.order = FALSE,
                 title.size = 1.5)
```

#### Bigram Networks

```{r}
yelp_bigrams <- yelp_sample %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  select(review_stars, word1, word2)
```

```{r}
neg_bigrams <- yelp_bigrams %>%
  filter(review_stars %in% c(1, 2)) %>%
  count(word1, word2) %>%
  filter(n > 150) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(neg_bigrams, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Bigram Network for One and Two Star Reviews") 
```

```{r}
pos_bigrams <- yelp_bigrams %>%
  filter(review_stars %in% c(4, 5)) %>%
  count(word1, word2) %>%
  filter(n > 400) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(pos_bigrams, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Bigram Network for Four and Five Star Reviews") 
```
