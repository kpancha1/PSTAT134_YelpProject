---
title: "PSTAT134_Yelp_Final"
format: pdf
editor: visual
---

# Final Project: NLP on Yelp Reviews

## PSTAT 134 - Group 19

### Installing Packages:

```{r, results='hide'}
library(tidyverse)
library(tidymodels)
library(tidytext)
library(kableExtra)
library(ggplot2)
library(stopwords)
library(wordcloud)
library(reshape2)
library(igraph)
library(ggraph)
```

### Load in Dataset:

```{r}
yelp <- read.csv("~/Downloads/dataset.csv")
```

Let's take a look at the entire dataset's distribution of `review_stars`, the number of stars the reviewer gave the business.

```{r}
yelp %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal()
```

This dataset has almost seven million observations. To work with the data, we will take a random sample of $10,000$ reviews, stratified by `review_stars`.

We stratify on `review_stars` to ensure that we are able to capture a representative distribution of reviews across all star ratings. Since `review_stars` reflects the sentiment or quality of the review (ranging from 1 star for very negative to 5 stars for very positive), stratifying by this variable guarantees that every rating level is proportionally included in the sample. This avoids over- or under-representing any particular rating category in the sample.

```{r}
yelp_sample <- yelp %>% 
  group_by(review_stars) %>%
  sample_frac(10000 / nrow(yelp)) %>%
  ungroup()
```

We see that the proportion below remains the same. Let's continue with data cleaning.

```{r}
yelp_sample %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  theme_minimal()
```

### Data Cleaning

Observing the csv file, we see that the `yelping_since` column give the specific date and time at which the user created their Yelp profile. We do not require such detailed information for our analysis so we will keep only the year the user joined Yelp.

```{r}
yelp_sample <- yelp_sample %>%
  mutate(yelping_since = substr(yelping_since, 1, 4)) # get only the year
```

We also want to clean the actual reviews so that we are able to properly perform NLP analysis on the text. That is done below by removing punctuation, symbols, extra white space, adding a space before capital letters, and replacing all uppercase with lowercase letters.

```{r}
# cleaning the reviews (text column)
remove <- c('[[:punct:]]', 
            '[[:digit:]]', 
            '[[:symbol:]]',
            'im', 'ive') %>%
  paste(collapse = '|')

yelp_sample$text <- yelp_sample$text %>% 
  str_remove_all('\'') %>%
  str_replace_all(remove, ' ') %>%
  str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
  tolower() %>%
  str_replace_all("\\s+", " ")
```

#### Removing Stop Words

```{r}
# using tidytext stop words lexicon
data("stop_words")

yelp_sample %>% 
  unnest_tokens(word, text) %>% # tokenizing reviews into words
  anti_join(stop_words) %>% # removing stop words
  count(word, sort = TRUE) %>% 
  head(n = 30) %>% 
  kbl() %>%
  scroll_box(width = "400px", height = "500px")
```

#### Most Common Words

```{r}
yelp_sample_tokenized <- yelp_sample %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

yelp_sample_tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()
```

```{r}
yelp_sample_tokenized %>%
  count(review_stars, word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder_within(word, n, review_stars)) %>%
  ggplot(aes(x = n, y = word, fill = review_stars)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review_stars, scales = "free_y", 
             labeller = labeller(review_stars = function(x) paste(x, "Stars"))) +
  scale_y_reordered() +
  labs(title = "Most Common Words by Reviewers' Star Rating",
       x = "Frequency",
       y = NULL) +
  theme_minimal()

```

#### Word Cloud

```{r}
yelp_sample_tokenized %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100)
```

#### Bigram Networks

```{r}
yelp_bigrams <- yelp_sample %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  select(label, word1, word2)
```

```{r}
pos_bigrams <- yelp_bigrams %>%
  filter(label == "POSITIVE") %>%
  count(word1, word2) %>%
  filter(n > 50) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(pos_bigrams, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Bigram Network for Positive Reviews") +
  theme_void()
```
