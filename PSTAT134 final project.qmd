---
title: "PSTAT134_Yelp_Final"
format: pdf
editor: visual
---

# Final Project: NLP on Yelp Reviews

## PSTAT 134 - Group 19

## Introduction

This project utilizes a precompiled dataset from Yelp known as the Yelp Open Dataset. This dataset is made for academic use and research. The data is in JSON format and includes 6,990,280 reviews from 150,346 businesses over 11 metropolitan areas. Additionally, there are about 900,000 tips from over a million users which include over 1.2 million business attributes like hours, parking, availability, and ambience. The goal of this data science project is to utilize natural language processing and perform exploratory data analysis to gain insights that can help reveal patterns and trends between reviewers/reviews and business star ratings, votes, business categories, and more. We are interested in finding what insights from reviews are impactful in determining review star ratings and thereby business star ratings.

## Methods

Due to the dataset being in JSON format, initial data cleaning was required. The file, \`get_data.R\`, retrieves the JSON file and converts the data into a more readable format as multiple CSV files. The script processes the JSON file in chunks of 1,000 lines and saves processed chunks of data into .rds files every 1,000,000 lines to prevent memory overload. 

The file, \`dataset.R\`, is the script in which the CSV files are unified and unnecessary variables are dropped, such as the \`friends\` column.  This file results in a final and cleaned \`cleaned_dataset.csv\` file with over six million observations (reviews). The variables in this file are review_stars, text, city, categories and states.

### Setup

#### Installing Packages

```{r, results='hide'}
rm(list=ls())
gc() 
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(stringr)
library(igraph)
library(data.table)
library(ggraph)
library(tidytext)
library(wordcloud)
library(stringr)
library(textTinyR)
library(tm)
library(e1071)
library(tidyr)
library(caret)
library(naniar)
library(shiny)
library(htmltools)
library(kableExtra)
library(tidyr)
library(stopwords)
library(reshape2)
library(gridExtra) 
library(dplyr)
library(RColorBrewer)
```

#### Load in Dataset

```{r}
yelp <- fread("cleaned_dataset(3).csv")
```

#### Checking NA Values

We see from the visualization below that our CSV has no missing values. Therefore we may continue to further analysis.

```{r}
vis_miss(yelp, warn_large_data = FALSE) 
```

### EDA

#### Distribution of Reviewers' Star Ratings

Let's take a look at the entire dataset's distribution of `review_stars`, the number of stars the reviewer gave the business.

```{r}
yelp %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +  
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.3) +
  theme_minimal()
```

This dataset has almost seven million observations. To work with the data, we will take a random sample of $10,000$ reviews, stratified by `review_stars`.

We stratify on `review_stars` to ensure that we are able to capture a representative distribution of reviews across all star ratings. Since `review_stars` reflects the sentiment or quality of the review (ranging from 1 star for very negative to 5 stars for very positive), stratifying by this variable guarantees that every rating level is proportionally included in the sample. This avoids over- or under-representing any particular rating category in the sample.

```{r}
yelp_sample <- yelp %>% 
  group_by(review_stars) %>%
  sample_frac(100000 / nrow(yelp)) %>%
  ungroup()

#yelp_sample
```

We see that the proportion below remains the same. We can now begin with data cleaning of the working dataset, `yelp_sample`.

```{r}
yelp_sample %>%
  ggplot(aes(x = review_stars)) +
  geom_bar() +
  labs(
    title = "Distribution of Reviewers' Star Rating",
    x = "Reviews' Star Ratings",
    y = "Count"
  ) +
  scale_y_continuous(labels = comma) +
  geom_text(stat = 'count', aes(label = after_stat(count)), vjust = -0.3) +
  theme_minimal()

```

#### Review counts for states

Let's examine the overall distribution of states, representing the locations of the restaurants.

```{r}
yelp %>% 
  count(state) %>%  
  filter(n > 50) %>%  
  ggplot(aes(x = reorder(state, n), y = n)) +  
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of States (More than 50 Counts)",
    x = "State",
    y = "Count") +
  geom_text(aes(label = n), hjust = -0.3) +  
  ylim(c(0,2000000)) +
  coord_flip()

```

This bar chart illustrates the distribution of restaurants across states, with only states having more than 50 counts displayed. Pennsylvania (PA) has the highest count of restaurants at 1,598,772, followed by Florida (FL) with 1,161,405. Other notable states include California (CA) and Nevada (NV), which have significantly fewer counts compared to the top states, highlighting a skewed distribution. This indicates that the data is biased toward certain states, so we should focus our analysis on the overall USA, restaurant categories rather than state-specific distributions.

#### The Five most popular restaurant categories.

Let's explore the overall distribution of categories, showcasing the various types of restaurants. We will only focus on categories categorized as restaurants. Since specific types like steakhouses and sushi bars have fewer entries, so we considered only the broader categories across all classifications.

```{r}
yelp_categories <- yelp %>%
  mutate(sorted_categories = sapply(strsplit(as.character(categories), ",\\s*"), function(x) paste(sort(x), collapse = " "))
  ) %>% mutate(sorted_categories = case_when(
  sorted_categories %in% c('Italian Pizza Restaurants') ~ 'Pizza Restaurants',
  T ~ sorted_categories
))%>% mutate(sorted_categories = case_when(
  sorted_categories %in% c("Japanese Restaurants Sushi Bars") ~ "Japanese Restaurants",
  T ~ sorted_categories
))

yelp_categories2 <- yelp_categories %>%
  filter(sorted_categories %in% c('Pizza Restaurants', 'Mexican Restaurants',
                                  "Chinese Restaurants", "Burgers Fast Food Restaurants",
                                  "Japanese Restaurants"))

top_sorted_categories <- yelp_categories2 %>%
  distinct(business_id, sorted_categories) %>%  
  count(sorted_categories, sort = TRUE)

ggplot(top_sorted_categories, aes(x = reorder(sorted_categories, n), y = n)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  geom_text(aes(label = n), position = position_stack(vjust = 0.5), color = "black") +
  labs(
    title = "Top 5 Sorted Restaurants Categories",
    x = "Sorted Categories",
    y = "Count"
  ) +
  coord_flip() +  
  theme_minimal() +
  theme(axis.text.y = element_text(size = 10)) 

```

The bar chart displays the top 5 sorted restaurant categories based on their frequency. Pizza Restaurants lead with 2,426 entries, followed by Mexican Restaurants (1,400), Chinese Restaurants (1,359), Burgers Fast Food Restaurants (815), and Japanese Restaurants (499).


# start
 
```{r}
top_cities <- yelp %>%
  count(city, sort = TRUE) %>%
  slice_head(n = 5) %>%
  pull(city)
```

```{r}
yelp |> 
  count(city,sort=T) |> 
  head(n=5) |> 
  ggplot(aes(x=fct_reorder(city,n) |> fct_rev(),y=n)) + 
  geom_col(aes(fill=city)) + 
  theme(axis.text.x=element_text(angle=45)) + 
  labs(
    title='Top 5 Cities by Number of Review',
    x='City',
    y='Count'
  )
```




```{r}
yelp_top_cities <- yelp %>% filter(city %in% top_cities)
```




```{r}
food_categories <- c("Mexican", "Chinese", "Italian", "Japanese",'French','Korean')

# Filter data for the selected categories
yelp_filtered <- yelp_top_cities %>%
  filter(str_detect(categories, paste(food_categories, collapse = "|")))

yelp_filtered |> 
  mutate(
    category=case_when(str_detect(categories,'Mexican') ~ 'Mexican',
                       str_detect(categories,'Chinese') ~ 'Chinese',
                       str_detect(categories,'Italian') ~ 'Italian',
                       str_detect(categories,'Japanese') ~ 'Japanese',
                       str_detect(categories,'Korean') ~ 'Korean',
                       str_detect(categories,'French') ~ 'French')
  ) -> yelp_filtered
  


yelp_filtered |> 
  ggplot(aes(x=category)) + 
  geom_bar()
```





```{r}
yelp_filtered |> 
  group_by(category) |> 
  count(review_stars) |> 
  mutate(n=n/sum(n)) |> 
  ggplot(aes(x=as.factor(review_stars),y=n)) + 
  geom_col() + 
  facet_wrap(~category,nrow=2) + 
  labs(
    x='Stars',
    title='Proportion of Reviews by Stars By Category'
  )


yelp_filtered %>%
  filter(review_stars==5) |> 
  group_by(city) |> 
  count(category) |> 
  mutate(n=n/sum(n)) |> 
  ggplot(aes(x=category,y=n)) + 
  geom_col() + 
  facet_wrap(~city) + 
  labs(
    x='Category',
    title='Proportion of 5-Star Reviews by Category'
  ) + 
  coord_flip()
```


```{r}
yelp_filtered |> 
  group_by(category,city) |> 
  count(review_stars) |> 
  mutate(n=n/sum(n)) |> 
  filter(review_stars == 1) |> 
  ggplot(aes(x=category,y=n)) + 
  geom_col() + 
  facet_wrap(~city) + 
  ylim(c(0,0.2)) + 
  labs(
    title='Percent of 1 Star Reviews by Category by City'
  ) + 
  coord_flip()
```


```{r}
yelp_filtered |> 
  ggplot(aes(x=review_stars,y=city))  + 
  ggridges::geom_density_ridges(aes(fill=city))


# Review Counts

yelp_filtered |> 
  count(city,category) |> 
  ggplot(aes(x=category,y=n)) + 
  geom_col() + 
  facet_wrap(~city,nrow=1) + 
  coord_flip()
```
  







```{r}
yelp_themes <- yelp_words %>%
  mutate(theme = case_when(
    word %in% c("taste", "delicious", "bland", "flavor") ~ "Taste",
    word %in% c("portion", "size", "small", "large") ~ "Portion Size",
    word %in% c("quality", "fresh", "bad", "stale") ~ "Quality",
    word %in% c("location", "close", "far") ~ "Location",
    word %in% c("ambiance", "quiet", "noisy") ~ "Ambiance",
    word %in% c("staff", "friendly", "rude") ~ "Staff Friendliness",
    word %in% c("clean", "dirty") ~ "Cleanliness",
    word %in% c("restroom", "bathroom") ~ "Restroom Conditions",
    word %in% c("delivery", "quick", "slow") ~ "Food Delivery Speed"
  )) |> 
  drop_na()

yelp_themes %>%
  count(theme, review_stars) %>%
  ggplot(aes(x = theme, y = n, fill = as.factor(review_stars))) +
  geom_col(position = "dodge") +
  labs(
    title = "Themes in 1-Star and 5-Star Reviews",
    x = "Themes",
    y = "Count"
  ) +
  theme_minimal() + 
  coord_flip()
```

# end 


### Analysis for the entire USA

#### Data Cleaning

Observing the csv file, we see that the `yelping_since` column give the specific date and time at which the user created their Yelp profile. We do not require such detailed information for our analysis so we will keep only the year the user joined Yelp.

```{r}
yelp_sample <- yelp_sample %>%
  mutate(yelping_since = substr(yelping_since, 1, 4)) 
`````

We also want to clean the actual reviews so that we are able to properly perform NLP analysis on the text. That is done below by removing punctuation, symbols, extra white space, adding a space before capital letters, and replacing all uppercase with lowercase letters.

```{r}
remove <- c('[[:punct:]]', 
            '[[:digit:]]', 
            '[[:symbol:]]',
            'im', 'ive', 'didnt') %>%
  paste(collapse = '|')

yelp_sample$text <- yelp_sample$text %>% 
  str_remove_all('\'') %>%
  str_replace_all(remove, ' ') %>%
  str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
  tolower() %>%
  str_replace_all("\\s+", " ")
```

#### Removing Stop Words

Global stop words are words such as "didn't", "of", and "or", for example.These words are very common and typically don’t add much to the meaning of a text so we can remove them to focus on relevant words. We use the tidytext stop words lexicon.

```{r}
data("stop_words")

yelp_sample %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  head(n = 30) %>% 
  kbl() %>% 
  scroll_box(width = "400px", height = "500px")
```

#### Most Common Words

Let's take a look at the top 20 most common words across all reviews.

```{r}
yelp_sample_tokenized <- yelp_sample %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)

word_counts <- yelp_sample_tokenized %>%
  count(word, sort = TRUE) %>%
  filter(n > 10000) %>%
  mutate(word = reorder(word, n))

ggplot(word_counts[1:20,], aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()
```

We now want to observe the difference between the most common words in lower rated (1-3 star) reviews and the most common words in higher rated (4-5 star) reviews.

```{r}
word_counts_by <- yelp_sample_tokenized %>%
  count(review_stars, word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder_within(word, n, review_stars))
  
word_counts_1to3 <- word_counts_by %>%
  filter(review_stars %in% 1:3)

plot_1to3 <- ggplot(word_counts_1to3[1:40, ], aes(x = n, y = word, fill = review_stars)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review_stars, scales = "free_y", 
             labeller = labeller(review_stars = function(x) paste(x, "Stars"))) +
  scale_y_reordered() +
  labs(title = "Most Common Words for Ratings 1 to 3",
       x = "Frequency",
       y = NULL) +
  theme_minimal()

print(plot_1to3)
```

```{r}
word_counts_4to5 <- word_counts_by %>%
  filter(review_stars %in% 4:5)

plot_4to5 <- ggplot(word_counts_4to5[1:40, ], aes(x = n, y = word, fill = review_stars)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~review_stars, scales = "free_y", 
             labeller = labeller(review_stars = function(x) paste(x, "Stars"))) +
  scale_y_reordered() +
  labs(title = "Most Common Words for Ratings 4 to 5",
       x = "Frequency",
       y = NULL) +
  theme_minimal()

print(plot_4to5)
```

#### Word Cloud

Word clouds allow us to visualize the most common words in a different way. The larger the word is shown, the more common it is. We have created a word cloud that distinguishes words based on negative or positive sentiment. The red words indicate positive sentiment and the blue words indicate negative sentiment.

```{r}
sentiment_counts <- yelp_sample_tokenized %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0)

comparison.cloud(sentiment_counts,
                 colors = c("red3", "navyblue"),
                 max.words = 50, 
                 scale = c(2.5, 0.5),
                 random.order = FALSE,
                 title.size = 1.5)
```

#### Bigram Networks

Bigram networks allow us to visualize bigrams (groups of 2 associated words). The weight of the arrows indicate the strength of the correlation between two words. We again will create two graphs – one for lower rated (1-3 star) reviews and one for higher rated (4-5 star) reviews.

```{r, eval = FALSE}
yelp_bigrams <- yelp_sample %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  select(review_stars, word1, word2)

saveRDS(yelp_bigrams, file = "yelp_bigrams.rds")
```

```{r}
yelp_bigrams <- readRDS("yelp_bigrams.rds")

neg_bigrams <- yelp_bigrams %>%
  filter(review_stars %in% c(1, 2, 3)) %>%
  count(word1, word2) %>%
  filter(n > 150) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(neg_bigrams, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Bigram Network for 1-3 Star Reviews") 
```

We see from the above bigram network for 1-3 star reviews that

```{r}
pos_bigrams <- yelp_bigrams %>%
  filter(review_stars %in% c(4, 5)) %>%
  count(word1, word2) %>%
  filter(n > 400) %>%
  graph_from_data_frame()

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(pos_bigrams, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "violet", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  labs(title = "Bigram Network for 4-5 Star Reviews") 
```

We see from the above bigram network for 4-5 star reviews that

### Analysis based on restaurant categories

#### Cleaning the data and eliminating stop words.

Global stop words are words such as "didn't", "of", and "or", for example.These words are very common and typically don’t add much to the meaning of a text so we can remove them to focus on relevant words. We use the tidytext stop words lexicon.

```{r}
filtered_yelp <- yelp_categories %>%
  filter(sorted_categories %in% top_sorted_categories$sorted_categories)

remove <- c('[[:punct:]]', '[[:digit:]]', '[[:symbol:]]', 'im', 'ive') %>%
  paste(collapse = '|') 

yelp_filtered_tokenized <- filtered_yelp %>%
  mutate(text = str_remove_all(text, '\''),
         text = str_replace_all(text, remove, ' '),
         text = str_replace_all(text, "([a-z])([A-Z])", "\\1 \\2"),
         text = tolower(text),
         text = str_replace_all(text, "\\s+", " ")) %>%
  unnest_tokens(word, text) %>%  
  anti_join(stop_words, by = "word")  
```

#### Word cloud representing positive reviews with 5-star ratings

We are creating word clouds for positive 5-star reviews, comparing pairs of restaurant categories (e.g., Pizza Restaurants vs. Mexican Restaurants) by highlighting positive words identified using the "bing" sentiment lexicon. Each category is assigned a distinct color, and the most frequent words are displayed for each pair in a grid of word cloud plots.

```{r}
categories <- c("Pizza Restaurants", "Mexican Restaurants", "Chinese Restaurants", 
                "Burgers Fast Food Restaurants","Japanese Restaurants")


restaurant_pairs <- combn(categories, 2, simplify = FALSE)


filtered_data <- yelp_filtered_tokenized %>%
  filter(
    sorted_categories %in% unlist(restaurant_pairs),
    review_stars == 5
  ) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  filter(sentiment == "positive")  

combined_data <- filtered_data %>%
  count(sorted_categories, word, sort = TRUE)

for (pair in restaurant_pairs) {
  category1 <- pair[1]
  category2 <- pair[2]
  
  pair_data <- combined_data %>%
    filter(sorted_categories %in% c(category1, category2))
  
  pair_data <- pair_data %>%
    mutate(color = case_when(
      sorted_categories == category1 ~ "blue",
      sorted_categories == category2 ~ "green"
    ))
  
  par(mar = c(0, 0, 3, 0))  
  wordcloud(
    words = pair_data$word,
    freq = pair_data$n,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    scale = c(2, 0.4),
    colors = pair_data$color
  )
  
  title(main = paste("Positive with 5-Star Reviews Word Cloud \n",
                     category1, "(Blue) vs. ", category2, "(Green)"),
        cex.main = 0.8)
}

```

For instance, when comparing Pizza Restaurants (blue) and Mexican Restaurants (green), the size of each word reflects its frequency, with larger words like "delicious," "amazing," and "fresh" standing out as the most commonly used descriptors. Overlapping words such as "friendly," "love," and "excellent" indicate shared positive sentiments across both restaurants. Additionally, unique words like "authentic" for Mexican Restaurants and "favorite" for Pizza Restaurants provide insights into specific customer preferences for each type.

#### Word cloud representing negative reviews with 1-star ratings

We are creating word clouds for negative 1-star reviews, comparing pairs of restaurant categories by highlighting negative words identified using the "bing" sentiment lexicon. Each category is assigned a distinct color, and the most frequent words are displayed for each pair in a grid of word cloud plots.

```{r}
filtered_data <- yelp_filtered_tokenized %>%
  filter(
    sorted_categories %in% unlist(restaurant_pairs),
    review_stars == 1
  ) %>%
  inner_join(get_sentiments("bing"), by = "word") %>%
  filter(sentiment == "negative")  

combined_data <- filtered_data %>%
  count(sorted_categories, word, sort = TRUE)

for (pair in restaurant_pairs) {
  category1 <- pair[1]
  category2 <- pair[2]
  
  pair_data <- combined_data %>%
    filter(sorted_categories %in% c(category1, category2))
  
  pair_data <- pair_data %>%
    mutate(color = case_when(
      sorted_categories == category1 ~ "blue",
      sorted_categories == category2 ~ "green"
    ))
  
  par(mar = c(0, 0, 3, 0))  
  wordcloud(
    words = pair_data$word,
    freq = pair_data$n,
    min.freq = 1,
    max.words = 100,
    random.order = FALSE,
    scale = c(2, 0.4),
    colors = pair_data$color
  )
  
  title(main = paste("Negative with 1-Star Reviews Word Cloud \n",
                     category1, "(Blue) vs. ", category2, "(Green)"),
        cex.main = 0.8)
}
```

For example, the last world cloud shows the most common negative words in 1-star reviews for Burgers Fast Food Restaurants (blue) and Japanese Restaurants (green). Larger words, such as "worst," "cold," "rude," "bad," and "terrible," indicate higher frequencies, reflecting significant customer dissatisfaction. Shared terms like "disappointed" and "horrible" highlight issues common to both categories. Unique complaints include "frozen" and "bland" for Burgers Fast Food Restaurants and "mediocre" or "filthy" for Japanese Restaurants. The visualization effectively distinguishes general and specific problems, offering insights into customer grievances for each type of restaurant.

### Result

### Conclusion

When analyzing Yelp data, food quality consistently emerges as the top priority for consumers when choosing a restaurant. Reviews frequently emphasize the taste, flavor, and authenticity of dishes, with particular attention given to standout items or specialties. The freshness of ingredients is often highlighted, especially when customers can discern high-quality or locally sourced products. Additionally, portion sizes and value for money are crucial, as diners expect meals that justify their cost.

Service quality is also important, with consumers placing high value on friendly, polite, and attentive staff. Reviews often detail interactions with servers, praising efficient service and the effective resolution of issues such as delays or incorrect orders. 

Cleanliness is a vital factor that cannot be overlooked; patrons rightfully demand immaculate dining areas, pristine restrooms, and clear adherence to safety protocols, especially in the post-pandemic landscape.

Ambiance plays a significant role in the decision-making process as well. Diners appreciate a comfortable and aesthetically pleasing environment, where elements like decor, music, and lighting create a welcoming atmosphere. Reviews typically mention whether a restaurant’s vibe is suitable for specific occasions, such as casual meet-ups, family outings, or romantic dinners.

Lastly, convenience factors such as location, parking availability, and ease of reservations can heavily influence dining decisions, with customers frequently noting these aspects in their reviews.
